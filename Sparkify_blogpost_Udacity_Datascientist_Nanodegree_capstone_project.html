<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Understanding Churn in music streaming platform, using Pyspark — A detailed analysis</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Understanding Churn in music streaming platform, using Pyspark — A detailed analysis</h1>
</header>
<section data-field="subtitle" class="p-summary">
Sparkify has both paid and free users, and they take multiple actions on the platform from accessing different pages, to playing different…
</section>
<section data-field="body" class="e-content">
<section name="4b9a" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3380" id="3380" class="graf graf--h3 graf--leading graf--title">Understanding Churn in music streaming platform, using Pyspark — A detailed analysis</h3></div><div class="section-inner sectionLayout--outsetColumn"><figure name="5ccd" id="5ccd" class="graf graf--figure graf--layoutOutsetCenter graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 568px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.00000000000001%;"></div><img class="graf-image" data-image-id="1*Ks7IPP6BtGjbbmTuhObi2w.png" data-width="2844" data-height="1564" data-is-featured="true" src="https://cdn-images-1.medium.com/max/1200/1*Ks7IPP6BtGjbbmTuhObi2w.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="bfb8" id="bfb8" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--figure"><span class="graf-dropCap">S</span>parkify has both paid and free users, and they take multiple actions on the platform from accessing different pages, to playing different songs. There are a few actions which they take and we do not like it. They are downgrading to a free account (for paid users), and cancelling the account altogether (for both paid and free users). We collect all this information as logs. Our aim is to understand the data, see the trends, use ML algorithms to determine who amongst the current users will more likely churn so that we can try different techniques to retain them even before they try to churn.</p><p name="8c5f" id="8c5f" class="graf graf--p graf-after--p">In this project, we will see how we can access huge data sets that lie across clusters using Spark via Python. The Pyspark package will help us do that. We will however work with a mini sample for this project whilst keeping in mind that the code can be used on the whole dataset as well. The results should remain the same for larger datasets too.</p><p name="ff22" id="ff22" class="graf graf--p graf-after--p">We start with loading the data, perform some exploratory data analysis, then move on to do some feature engineering and finally, create multiple models and evaluate them to see which one is giving us better results in determining if a user will churn.</p><p name="81ba" id="81ba" class="graf graf--p graf-after--p graf--trailing"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Disclaimer</em></strong>: This project has been done as a Capstone project for the Udacity Data Scientist Nanodegree program. All data has been provided by Udacity, and is done to strengthen and display understanding of Pyspark, a Python programming interface for Apache Spark. Sparkify company in discussion here is a fictitious music streaming company. Github repository link <a href="https://github.com/yvsajay/Sparkify" data-href="https://github.com/yvsajay/Sparkify" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p></div></div></section><section name="c0e9" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="4917" id="4917" class="graf graf--h4 graf--leading"><strong class="markup--strong markup--h4-strong">Load and Clean data set</strong></h4><p name="5832" id="5832" class="graf graf--p graf-after--h4">We start by loading the data using <code class="markup--code markup--p-code">spark.read.json</code> as a spark dataframe and check for total records using <code class="markup--code markup--p-code">count()</code> function. We have 286500 rows in our mini dataset.</p><figure name="1499" id="1499" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 165px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 23.599999999999998%;"></div><img class="graf-image" data-image-id="1*c8n-o1uJeA7EtLkWq0Kmow.png" data-width="1332" data-height="314" src="https://cdn-images-1.medium.com/max/800/1*c8n-o1uJeA7EtLkWq0Kmow.png"></div><figcaption class="imageCaption">df.show(2)</figcaption></figure><p name="d2cb" id="d2cb" class="graf graf--p graf-after--figure graf--trailing">The next step is to look for duplicates or any rows without our key identifier which are ‘userId’ and ‘sessionId’. Whilst we see that there are no Nulls, or Blank strings for sessionIds, there are few Blanks for userIds. If we think for a second, we can see why that’s the case with the dataset. There are users on the Sparkify platform who access different pages even before they register. So, I went ahead and removed the blanks. We now have 278154 rows.</p></div></div></section><section name="63aa" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="f48f" id="f48f" class="graf graf--h4 graf--leading">Exploratory Data Analysis</h4><p name="1b6e" id="1b6e" class="graf graf--p graf-after--h4">I wanted to look and see if there’s any significant difference in listening preferences in terms of artists between the free and paid users. So, I wanted to capture the top 10 artists listened by all, free and paid users.</p><figure name="1001" id="1001" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 272px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.9%;"></div><img class="graf-image" data-image-id="1*ZcBv_tycZXYoBCto_xFLSA.png" data-width="1292" data-height="502" src="https://cdn-images-1.medium.com/max/800/1*ZcBv_tycZXYoBCto_xFLSA.png"></div><figcaption class="imageCaption">Top Artists — Sparkify</figcaption></figure><figure name="d574" id="d574" class="graf graf--figure graf--iframe graf-after--figure"><script src="https://gist.github.com/yvsajay/414547654c33247f4bc23985040495f3.js"></script></figure><p name="ca01" id="ca01" class="graf graf--p graf-after--figure">Next up, is identifying all the userId records where the user has already churned. So, we create a column to identify the same for us.</p><figure name="47d4" id="47d4" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvsajay/d781f8d2edf038088fc1c1057dfcecdb.js"></script></figure><p name="d145" id="d145" class="graf graf--p graf-after--figure">Now I wanted to see if there are any gender correlations in the data.</p><pre name="d4c6" id="d4c6" class="graf graf--pre graf-after--p">df.dropDuplicates([‘gender’,’userid’]).groupby(‘gender’,’churn’).count().sort(‘churn’).show()</pre><figure name="681a" id="681a" class="graf graf--figure graf--layoutOutsetLeft graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 234px; max-height: 178px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 76.1%;"></div><img class="graf-image" data-image-id="1*sl_ONmFDDWhxjd_ox4xtVw.png" data-width="234" data-height="178" src="https://cdn-images-1.medium.com/max/600/1*sl_ONmFDDWhxjd_ox4xtVw.png"></div></figure><p name="289f" id="289f" class="graf graf--p graf-after--figure">We see that there is no over dependence or influence of gender in determining churn. However, you’ll see later that we will still incorporate gender as one of our input features for our models.</p><p name="8276" id="8276" class="graf graf--p graf-after--p">We see that there is no over dependence or influence of gender in determining churn. However, you’ll see later that we will still incorporate gender as one of our input features for our models.</p><p name="d6a0" id="d6a0" class="graf graf--p graf-after--p">Next up, I wanted to compare the daily listening patterns in a week between the churned users and not churned users.</p><figure name="c196" id="c196" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 683px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 130.1%;"></div><img class="graf-image" data-image-id="1*Mu5v8_AH-Bn9xN1Pq7N7Nw.png" data-width="532" data-height="692" src="https://cdn-images-1.medium.com/max/600/1*Mu5v8_AH-Bn9xN1Pq7N7Nw.png"></div></figure><blockquote name="cae1" id="cae1" class="graf graf--pullquote graf-after--figure">We see that the listening patterns remained consistent between both groups with listening staying high during weekdays with a minor dip on Wednesdays. Weekend counts remained low as expected.</blockquote><p name="6510" id="6510" class="graf graf--p graf-after--pullquote">To obtain these graphs, we first create a column Weekday, and then sum up the counts as needed.</p><p name="8e2a" id="8e2a" class="graf graf--p graf-after--p">Code for this is provided below.</p><figure name="b62f" id="b62f" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><script src="https://gist.github.com/yvsajay/414547654c33247f4bc23985040495f3.js"></script></figure></div></div></section><section name="fbc2" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="a6e0" id="a6e0" class="graf graf--h4 graf--leading">Feature Engineering</h4><p name="6880" id="6880" class="graf graf--p graf-after--h4">This is a very critical step in any Data science related task. The features we engineer will most likely impact the efficiency of our models directly. This is one of the reasons why domain knowledge and understanding is important for any data scientist and it cannot be stressed enough.</p><p name="13cf" id="13cf" class="graf graf--p graf-after--p">I’ve chosen a variety of features for this project with an underlying hypothesis for each of them. Let me list them down here. You can find the underlying code for each of these features in my notebook that can be found <a href="https://github.com/yvsajay/Sparkify" data-href="https://github.com/yvsajay/Sparkify" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><ol class="postList"><li name="906a" id="906a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Average number of songs listened by a user in each session</strong><br>Hypothesis: If a user is there for longer duration in each session he/she will see more value and benefits of Sparkify and is less likely to churn.</li><li name="9424" id="9424" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Total number of sessions of a user</strong><br>Hypothesis: If the user is less frequent on the platform, he/she will probably churn.</li><li name="33d2" id="33d2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Average number of different Pages visited in each session</strong><br>Hypothesis: If the user is less exploring, he/she will probably not churn as he’s staying on single page, probably NextPage.</li><li name="cd83" id="cd83" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Gender</strong><br>Hypothesis: Maybe gender has an implicit bias.</li><li name="e942" id="e942" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Current User level — Paid or free<br></strong>Hypothesis: User is likely to stay longer if the user is a paid user.</li><li name="3ca6" id="3ca6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Average session time for a user</strong><br>Hypothesis: More the average session time for a user, he/she is less likely to churn from the platform.</li></ol><figure name="c80d" id="c80d" class="graf graf--figure graf--layoutOutsetLeft graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 260px; max-height: 504px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 193.79999999999998%;"></div><img class="graf-image" data-image-id="1*vZya-Hu7Ur7QthixfyylZA.png" data-width="260" data-height="504" src="https://cdn-images-1.medium.com/max/600/1*vZya-Hu7Ur7QthixfyylZA.png"></div><figcaption class="imageCaption">different pages available in Sparkify</figcaption></figure><p name="5cef" id="5cef" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">7. Number of times user visited different pages in total<br></strong>Hypothesis: The way user browses different pages on Sparkify may give us some insight into which users will churn depending on the different pages he chooses to access.<br><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Note: </em></strong><em class="markup--em markup--p-em">This feature will create multiple features, 1 for each different Page type. Also, w</em>e will create features only for those Pages which are generic and <strong class="markup--strong markup--p-strong">not</strong> one time in nature, like Downgrade, or Cancellation.</p><p name="0395" id="0395" class="graf graf--p graf-after--p">8. <strong class="markup--strong markup--p-strong">If a user has downgraded</strong><br>Hypothesis: A downgraded user will more likely churn.</p><p name="bbc0" id="bbc0" class="graf graf--p graf-after--p">9. <strong class="markup--strong markup--p-strong">Average time between sessions<br></strong>Hypothesis: More the average time between sessions, higher the chances of churn.</p><p name="d776" id="d776" class="graf graf--p graf-after--p">10. <strong class="markup--strong markup--p-strong">Ratio of average time between sessions during later stages vs initial stages<br></strong>Hypothesis: Building on previous point, while average gives a good idea of the frequency of a user coming to Sparkify, it will not tell us if the user has recently started to become frequent or less frequent. This may give us some insight if the user is going to churn.</p><p name="4077" id="4077" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">11. Output Label: Churn</strong></p><p name="6233" id="6233" class="graf graf--p graf-after--p">Before we complete our engineering, we merge all these different features on userId, replace NaNs with 0s, and save our final dataframe as a CSV for future ready access during a different session. Remember that this saving as CSV works here because of the small data set. We will ideally have to save this as a table in our working environment, whether it’s in AWS, or IBM Watson, or any other spaces.</p><figure name="f715" id="f715" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvsajay/f31a9e2e1278f5b181ba4ec42cc601eb.js"></script></figure><p name="d845" id="d845" class="graf graf--p graf-after--figure graf--trailing">We have 225 unique users/row out of which 52 have churned (positives).</p></div></div></section><section name="d25f" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3e52" id="3e52" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Modeling</strong></h3><p name="3b3d" id="3b3d" class="graf graf--p graf-after--h3">We split the data 80–20 for train-test, and then use pipelines for all of our models. The pipeline steps include, VectorAssembler, StandardScalar, and String Indexer (for the output column), and then the model.</p><figure name="5292" id="5292" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvsajay/58ba8497ac38bd1749002028c8abb37a.js"></script></figure><p name="dbc2" id="dbc2" class="graf graf--p graf-after--figure">We used <strong class="markup--strong markup--p-strong">4 models — Logistic Regression, Gradient Boost Tree Classifier, Random Forests Classifier, and Support Vector Classifier. </strong>For each of the models, we first tried to implement the pipeline directly and then again with hyperparameter tuning and cross validation. We maintained the number of folds at 3 for this exercise.</p><p name="82bd" id="82bd" class="graf graf--p graf-after--p">Code for the model with best results SVC classifier provided below. Please <a href="https://github.com/yvsajay/Sparkify" data-href="https://github.com/yvsajay/Sparkify" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">click here for the complete code.</a></p><figure name="9ff0" id="9ff0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvsajay/6a191eea2fe18039f7186a26d74b695e.js"></script></figure><h3 name="913c" id="913c" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Evaluation Metrics</strong></h3><p name="15d9" id="15d9" class="graf graf--p graf-after--h3">I decided to focus on F1-score as the metric to optimize as we have relatively very small number of positives. AUC ROC could have been used in cases where we have a good number of positives as it relies on True positive, and False positive counts.</p><p name="88bc" id="88bc" class="graf graf--p graf-after--p">Let us see how each of the above mentioned models performed post hyperparameter tuning and cross validation.</p><figure name="715c" id="715c" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 366px; max-height: 106px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.999999999999996%;"></div><img class="graf-image" data-image-id="1*reVAx75mFNAf8qRMfibY4g.png" data-width="366" data-height="106" src="https://cdn-images-1.medium.com/max/600/1*reVAx75mFNAf8qRMfibY4g.png"></div></figure><blockquote name="a3df" id="a3df" class="graf graf--pullquote graf-after--figure">Logistic Regression</blockquote><figure name="922e" id="922e" class="graf graf--figure graf--layoutOutsetLeft graf-after--pullquote"><div class="aspectRatioPlaceholder is-locked" style="max-width: 346px; max-height: 108px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.2%;"></div><img class="graf-image" data-image-id="1*8TJgySF4c3R-VOJMBDm8FA.png" data-width="346" data-height="108" src="https://cdn-images-1.medium.com/max/600/1*8TJgySF4c3R-VOJMBDm8FA.png"></div></figure><blockquote name="5844" id="5844" class="graf graf--pullquote graf-after--figure">GBT Classifier</blockquote><figure name="f83b" id="f83b" class="graf graf--figure graf--layoutOutsetLeft graf-after--pullquote"><div class="aspectRatioPlaceholder is-locked" style="max-width: 350px; max-height: 100px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.599999999999998%;"></div><img class="graf-image" data-image-id="1*oxiTGj5gwjapM1TW0mw1BA.png" data-width="350" data-height="100" src="https://cdn-images-1.medium.com/max/600/1*oxiTGj5gwjapM1TW0mw1BA.png"></div></figure><blockquote name="816c" id="816c" class="graf graf--pullquote graf-after--figure">Random Forest Classifier</blockquote><figure name="c9cb" id="c9cb" class="graf graf--figure graf--layoutOutsetLeft graf-after--pullquote"><div class="aspectRatioPlaceholder is-locked" style="max-width: 350px; max-height: 108px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.9%;"></div><img class="graf-image" data-image-id="1*CVnnmBIK8SOh3Ein8mSdvA.png" data-width="350" data-height="108" src="https://cdn-images-1.medium.com/max/600/1*CVnnmBIK8SOh3Ein8mSdvA.png"></div></figure><blockquote name="c590" id="c590" class="graf graf--pullquote graf-after--figure graf--trailing">Support Vector Classifier</blockquote></div></div></section><section name="d504" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4c84" id="4c84" class="graf graf--h3 graf--leading">Discussion on Models and their Results</h3><p name="06a7" id="06a7" class="graf graf--p graf-after--h3">We see that our best model in term’s of our agreed upon metric F1 score is <strong class="markup--strong markup--p-strong">Support Vector Classifier.</strong> We have achieved 82% accuracy and 57% F1 score on the test data post hyperparameter tuning and cross validation.</p><p name="9ce9" id="9ce9" class="graf graf--p graf-after--p">We have tried to use the direct model without any hyperparameter tuning or cross validation for each of the models and then refined them using tuning of parameters and cross validation. We chose Cross Validation over TrainValidationSplit as we have a small dataset and Cross validation generally provides a better fit for Classification purposes.</p><p name="1d98" id="1d98" class="graf graf--p graf-after--p">Also note that we did explicitly call out best model for each of the models post validation, which is not necessary as the models choose best model for estimation purposes by default. The reason why I did it is only to save my best model before I close my session as the project spanned across multiple days.</p><p name="1949" id="1949" class="graf graf--p graf-after--p">Few interesting details I noticed about the models:</p><p name="14f9" id="14f9" class="graf graf--p graf-after--p graf--trailing">* I wonder if <strong class="markup--strong markup--p-strong">Support Vector Classifier</strong> will give us even better results if we increase our <strong class="markup--strong markup--p-strong">maxIter</strong> parameter. However, it is important to note that SVC is very time consuming and has taken significantly higher amount of time to train on this small dataset relatively when compared with the other models. The current best model has maxIter 200, and regParam 0.01. It has no intercept.<br>* Logistic regression got a better F1 score without any hyperparameter tuning. It has got an F1 score of 50% without any tuning whilst only getting 47% post tuning.<br>* Random Forests, without hyperparameter tuning got zero Precision and Recall (because of which I have to edit my evaluate_model function too to include exception handling for F1 score). Best model had 30 Trees and maxDepth 8. This is because of having zero true positives. The F1 score improved later to 26% upon refinement of the model using tuning and cross validation.<br>* Gradient Boosting Trees has remained at an F1 score of 19% with or without tuning implying that the default parameters have been the best for this dataset. The best model has maxDepth 5, maxIter 50, and stepSize 0.1</p></div></div></section><section name="a9ed" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="ccc2" id="ccc2" class="graf graf--h3 graf--leading">Discussion on Features and their importances</h3><figure name="6bdc" id="6bdc" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 484px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69.1%;"></div><img class="graf-image" data-image-id="1*25AYhQThczypDtQA4ZoEYw.png" data-width="1102" data-height="762" src="https://cdn-images-1.medium.com/max/800/1*25AYhQThczypDtQA4ZoEYw.png"></div><figcaption class="imageCaption">Features mapped to their coefficients (SVC Classifier)</figcaption></figure><p name="be27" id="be27" class="graf graf--p graf-after--figure">The dominant features remained consistent across models.<br>In our winning model SVC, the features playing an important role in deciding whether a user will Churn or not are <br>* <strong class="markup--strong markup--p-strong">Average time between sessions</strong>. It is by far the most dominant feature. It is inversely correlated as expected. Users having more time in between sessions see less value in paying or using a platform which has a recurring monthly subscription. Even if he/she doesn’t pay, they might feel the platform is not needed for them anymore.<br>* <strong class="markup--strong markup--p-strong">Session Count</strong>, <strong class="markup--strong markup--p-strong">Add friend count</strong>, and <strong class="markup--strong markup--p-strong">Add to playlist count</strong> also had a surprisingly negative effect which is contradicting our hypothesis.<br>* <strong class="markup--strong markup--p-strong">Ratio recent count to initial count</strong> for which we had to feature engineer the field with more difficulty than the others had positive effect as expected. However, the extent of it was less than what we expected.</p><p name="e4ec" id="e4ec" class="graf graf--p graf-after--p graf--trailing">Finally, 22 features is fine to work with our models given the relatively small dataset that we worked for our project. However I strongly believe we should use PCA for feature selection before proceeding with any of our models if we want to deploy it on any large datasets.</p></div></div></section><section name="76dd" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4a3a" id="4a3a" class="graf graf--h3 graf--leading">Appendix — <strong class="markup--strong markup--h3-strong">Packages Needed</strong></h3><p name="49d8" id="49d8" class="graf graf--p graf-after--h3">Please perform <code class="markup--code markup--p-code">pip install &lt;package name&gt;</code> or <code class="markup--code markup--p-code">conda install &lt;package name&gt;</code> before performing the below imports as needed.</p><figure name="ff8b" id="ff8b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/yvsajay/cf2f6cdbb15ce6c524252e8aa6881052.js"></script></figure><h3 name="0c49" id="0c49" class="graf graf--h3 graf-after--figure">Acknowledgements</h3><p name="37f3" id="37f3" class="graf graf--p graf-after--h3">I would like to take this opportunity to thank Udacity for providing me this data. For those that have read till this point, thank you and for those interested, Udacity’s Data Scientist Nanodegree is a winner amongst most of the online courses, and it’s fit to present day expectations is close to 100%.</p><figure name="dbd5" id="dbd5" class="graf graf--figure graf-after--p graf--trailing"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 467px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.7%;"></div><img class="graf-image" data-image-id="0*hUCLQuR8jZCUgwSg" data-width="4641" data-height="3094" data-unsplash-photo-id="0YbeoQOX89k" src="https://cdn-images-1.medium.com/max/800/0*hUCLQuR8jZCUgwSg"></div><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@hannynaibaho?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@hannynaibaho?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Hanny Naibaho</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@yvsajay" class="p-author h-card">Ajay Yvs</a> on <a href="https://medium.com/p/8059ce3195b5"><time class="dt-published" datetime="2019-07-30T23:21:40.907Z">July 30, 2019</time></a>.</p><p><a href="https://medium.com/@yvsajay/understanding-churn-in-music-streaming-platform-using-pyspark-a-detailed-analysis-8059ce3195b5" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 31, 2019.</p></footer></article></body></html>